{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(DeepLearning assigment no: 15)"
      ],
      "metadata": {
        "id": "C57nOMtuemvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Deep Learning.\n",
        "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the\n",
        "ELU activation function.\n",
        "\n",
        "ANS: \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "d. Now try adding Batch Normalization and compare the learning curves: is it\n",
        "converging faster than before? Does it produce a better model?\n",
        "e. Is the model overfitting the training set? Try adding dropout to every layer and try\n",
        "again. Does it help?"
      ],
      "metadata": {
        "id": "XrXrTWVGerJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 100\n",
        "n_hidden2 = 100\n",
        "n_hidden3 = 100\n",
        "n_hidden4 = 100\n",
        "n_hidden5 = 100\n",
        "n_outputs = 5\n",
        "\n",
        "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "from functools import partial\n",
        "\n",
        "dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)\n",
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
        "\n",
        "with tf.name_scope('dnn'):\n",
        "    hidden1 = dense_layer(X, n_hidden1, name='hidden1')\n",
        "    hidden2 = dense_layer(hidden1, n_hidden2, name='hidden2')\n",
        "    hidden3 = dense_layer(hidden2, n_hidden3, name='hidden3')\n",
        "    hidden4 = dense_layer(hidden3, n_hidden4, name='hidden4')\n",
        "    hidden5 = dense_layer(hidden4, n_hidden5, name='hidden5')\n",
        "    logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
        "    \n",
        "with tf.name_scope('loss'):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name='loss')"
      ],
      "metadata": {
        "id": "3e0uhrpUrbTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later."
      ],
      "metadata": {
        "id": "yhIsOdj-rrco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "with tf.name_scope('train'):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "with tf.name_scope('eval'):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('/tmp/data/')\n",
        "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
        "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
        "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
        "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
        "X_train = mnist.train.images[mnist.train.labels < 5]\n",
        "y_train = mnist.train.labels[mnist.train.labels < 5]\n",
        "X_test = mnist.test.images[mnist.test.labels < 5]\n",
        "y_test = mnist.test.labels[mnist.test.labels < 5]\n",
        "def shuffle_split(X, y, n_batches):\n",
        "    np.random.seed(seed=42)\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    for i_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch = X[i_idx]\n",
        "        y_batch = y[i_idx]\n",
        "        yield X_batch, y_batch\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 50\n",
        "n_batches = len(X_train) // batch_size\n",
        "best_loss = float('inf')\n",
        "patience = 2\n",
        "cnt_patience = 0\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_split(X_train, y_train, n_batches):\n",
        "            sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        loss_test = loss.eval(feed_dict={X: X_test, y: y_test})\n",
        "        print(epoch, 'loss', loss_test, 'accuracy_train:', accuracy_train, 'accuracy_test:', accuracy_test)\n",
        "        if loss_test < best_loss:\n",
        "            best_loss = loss_test\n",
        "        else:\n",
        "            cnt_patience += 1\n",
        "            if cnt_patience > patience:\n",
        "                'Early stopping!'\n",
        "                break        "
      ],
      "metadata": {
        "id": "SEu1rUQDrzCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3:Now try adding Batch Normalization and compare the learning curves: is it\n",
        "converging faster than before? Does it produce a better model?\n",
        "\n",
        "Batch size, as it will largely affect the training time of future experiments.\n",
        "\n",
        "Architecture of the network:\n",
        "\n",
        "Number of neurons in the network\n",
        "Number of layers\n",
        "Rest (dropout, L2 reg, etc.)\n",
        "\n",
        "Dependencies:\n",
        "\n",
        "I'd assume that the optimal values of\n",
        "\n",
        "learning rate and batch size\n",
        "learning rate and number of neurons\n",
        "number of neurons and number of layers\n",
        "strongly depend on each other. I am not an expert on that field though."
      ],
      "metadata": {
        "id": "-g-ZM7GKr9ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DnnClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \n",
        "    \n",
        "    def __init__(self, \n",
        "                 batch_size=50, \n",
        "                 n_neuron=100):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.n_neuron = n_neuron\n",
        "        \n",
        "    def reset_graph(self, seed=42):\n",
        "        tf.reset_default_graph()\n",
        "        tf.set_random_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "    def _build_graph(self, n_inputs, n_outputs):\n",
        "        \n",
        "        self.reset_graph()\n",
        "        \n",
        "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
        "        y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
        "        \n",
        "        he_init = tf.contrib.layers.variance_scaling_initializer()\n",
        "        \n",
        "        dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)\n",
        "\n",
        "        with tf.name_scope('dnn'):\n",
        "            hidden1 = dense_layer(X, self.n_neuron, name='hidden1')\n",
        "            hidden2 = dense_layer(hidden1, self.n_neuron, name='hidden2')\n",
        "            hidden3 = dense_layer(hidden2, self.n_neuron, name='hidden3')\n",
        "            hidden4 = dense_layer(hidden3, self.n_neuron, name='hidden4')\n",
        "            hidden5 = dense_layer(hidden4, self.n_neuron, name='hidden5')\n",
        "            logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
        "            \n",
        "        with tf.name_scope('softmax'):\n",
        "            y_proba = tf.nn.softmax(logits, axis=1, name='y_proba')\n",
        "    \n",
        "        with tf.name_scope('loss'):\n",
        "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "            loss = tf.reduce_mean(xentropy, name='loss')\n",
        "        \n",
        "        learning_rate = 0.001\n",
        "\n",
        "        with tf.name_scope('train'):\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "            training_op = optimizer.minimize(loss)\n",
        "            \n",
        "        with tf.name_scope('eval'):\n",
        "            correct = tf.nn.in_top_k(logits, y, 1)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "            \n",
        "        self._training_op = training_op\n",
        "        self._accuracy = accuracy\n",
        "        self._X = X\n",
        "        self._y = y\n",
        "        self._logits = logits\n",
        "        self._y_proba = y_proba\n",
        "        \n",
        "    def shuffle_split(self, X, y, n_batches):\n",
        "        np.random.seed(seed=42)\n",
        "        rnd_idx = np.random.permutation(len(X))\n",
        "        for i_idx in np.array_split(rnd_idx, n_batches):\n",
        "            X_batch = X[i_idx]\n",
        "            y_batch = y[i_idx]\n",
        "            yield X_batch, y_batch\n",
        "        \n",
        "    def fit(self, X, y, n_epochs=5, X_valid=None, y_valid=None):\n",
        "        self.n_batches = len(X) // self.batch_size\n",
        "        \n",
        "        n_inputs = X.shape[1]\n",
        "        self.n_outputs = len(np.unique(y))\n",
        "        self.classes_ = np.unique(y)\n",
        "        \n",
        "        self._build_graph(n_inputs, self.n_outputs)\n",
        "        \n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "        self._session = tf.Session()\n",
        "        with self._session.as_default() as sess:\n",
        "            init.run()\n",
        "            for epoch in range(n_epochs):\n",
        "                for X_batch, y_batch in self.shuffle_split(X, y, self.n_batches):\n",
        "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
        "                    sess.run([self._training_op], feed_dict=feed_dict)\n",
        "                accuracy_train = self._accuracy.eval(feed_dict=feed_dict)\n",
        "                if X_valid is not None and y_valid is not None:\n",
        "                    accuracy_valid = self._accuracy.eval(feed_dict={self._X: X_valid, self._y: y_valid})\n",
        "                    print('epoch:', epoch, 'accuracy_valid:', accuracy_valid*100)\n",
        "                else:\n",
        "                    print('epoch:', epoch, 'accuracy_train:', accuracy_train*100)\n",
        "                \n",
        "                \n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        with self._session.as_default() as sess:\n",
        "            y_proba = sess.run([self._y_proba], feed_dict={self._X: X})\n",
        "            return np.array(y_proba).reshape((len(X), self.n_outputs))\n",
        "        \n",
        "    def predict(self, X):\n",
        "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
        "        return np.array([[self.classes_[class_index]]\n",
        "                         for class_index in class_indices], np.int32)\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "WbFoj5hmsDcw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}