{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(DeepLearning assigment no: 12)"
      ],
      "metadata": {
        "id": "7ZbZontABLi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: How does unsqueeze help us to solve certain broadcasting problems?\n",
        "\n",
        "ANS: unsqueeze is a method to change the tensor dimensions, such that operations such as tensor multiplication can be possible. This basically alters the dimension to produce a tensor that has a different dimension.\n",
        "\n",
        "Q2: How can we use indexing to do the same operation as unsqueeze?\n",
        "\n",
        "ANS: Import the required library. In all the following Python examples, the required Python library is torch. Make sure you have already installed it.\n",
        "\n",
        "Create a tensor and print it.\n",
        "\n",
        "Compute torch.squeeze(input). It squeezes (removes) the size 1 and returns a tensor with all other dimensions of the input tensor.\n",
        "\n",
        "Compute torch.unsqueeze(input, dim). It inserts a new dimension of size 1 at the given dim and returns the tensor.\n",
        "\n",
        "Print the squeezed and/or unsqueezed tensor.\n",
        "\n",
        "Q3: How do we show the actual contents of the memory used for a tensor?\n",
        "\n",
        "ANS: For GPUs, TensorFlow will allocate all the memory by default, unless changed with tf.config.experimental.set_memory_growth. This function only returns the memory that TensorFlow is actually using, not the memory that TensorFlow has allocated on the GPU.\n",
        "\n",
        "Q4: Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
        "\n",
        "ANS: Broadcasting should not increase the memory usage, but you would of course need to store the result. \n",
        "\n",
        "Q5: Implement matmul using Einstein summation.\n",
        "\n",
        "ANS: The einsum() method evaluates the Einstein summation convention on the operands. Using the Einstein summation convention, many common multi-dimensional, linear algebraic array operations can be represented in a simple fashion. In implicit mode einsum computes these values.\n",
        "\n",
        "In explicit mode, einsum provides further flexibility to compute other array operations that might not be considered classical Einstein summation operations, by disabling, or forcing summation over specified subscript labels."
      ],
      "metadata": {
        "id": "BuZ2bnTRBUtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Creating two numpy One-Dimensional array using the array() method\n",
        "arr1 = np.arange(25).reshape(5,5)\n",
        "arr2 = np.arange(5)\n",
        "\n",
        "# Display the arrays\n",
        "print(\"Array1...\\n\",arr1)\n",
        "print(\"\\nArray2...\\n\",arr2)\n",
        "\n",
        "# Check the Dimensions of both the arrays\n",
        "print(\"\\nDimensions of Array1...\\n\",arr1.ndim)\n",
        "print(\"\\nDimensions of Array2...\\n\",arr2.ndim)\n",
        "\n",
        "# Check the Shape of both the arrays\n",
        "print(\"\\nShape of Array1...\\n\",arr1.shape)\n",
        "print(\"\\nShape of Array2...\\n\",arr2.shape)\n",
        "\n",
        "# For Matrix Vector multiplication with Einstein summation convention, use the numpy.einsum() method in Python.\n",
        "print(\"\\nResult (Matrix Vector multiplication)...\\n\",np.einsum('ij,j', arr1, arr2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqYi6hcQEttE",
        "outputId": "964a77a9-d813-4a88-c46c-6cc7d19b1ceb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array1...\n",
            " [[ 0  1  2  3  4]\n",
            " [ 5  6  7  8  9]\n",
            " [10 11 12 13 14]\n",
            " [15 16 17 18 19]\n",
            " [20 21 22 23 24]]\n",
            "\n",
            "Array2...\n",
            " [0 1 2 3 4]\n",
            "\n",
            "Dimensions of Array1...\n",
            " 2\n",
            "\n",
            "Dimensions of Array2...\n",
            " 1\n",
            "\n",
            "Shape of Array1...\n",
            " (5, 5)\n",
            "\n",
            "Shape of Array2...\n",
            " (5,)\n",
            "\n",
            "Result (Matrix Vector multiplication)...\n",
            " [ 30  80 130 180 230]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: What are the three rules of Einstein summation notation? Why?\n",
        "\n",
        "ANS: The “rules” of summation convention are:\n",
        "\n",
        " Each index can appear at most twice in any term.\n",
        " \n",
        "  Repeated indices are implicitly summed over. \n",
        "  \n",
        "  Each term must contain identical non-repeated indices.\n",
        "\n",
        "   Einstein summation convention or Einstein summation notation) is a notational convention that implies summation over a set of indexed terms in a formula, thus achieving brevity.\n",
        "\n",
        "Q7: Why do we need to store some of the activations calculated for intermediate layers in the\n",
        "forward pass?\n",
        "\n",
        "ANS: we need to store all intermediate activations during training because they are needed to compute gradients during back-propagation: (5)-(8) involve not just the values of the incoming gradient, but also the values of the activations themselves.\n",
        "\n",
        "Q8: How can weight initialization help avoid this problem?\n",
        "\n",
        "ANS:  building a neural network, it's important to initialize weights beforehand. Weights can be thought of as the amount of influence the input has on the output. With weight initialization, we set the weights to random values to prevent the layer outputs or gradients from vanishing or exploding.\n",
        "\n"
      ],
      "metadata": {
        "id": "I5Sc077uFE7Y"
      }
    }
  ]
}