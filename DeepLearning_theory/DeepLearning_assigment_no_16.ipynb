{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(DeepLearning assigment no: 16)"
      ],
      "metadata": {
        "id": "rNJJesMNuySv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What happens when you increase or decrease the optimizer learning rate?\n",
        "\n",
        "ANS: A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.\n",
        "\n",
        "\n",
        "Q2: What happens when you increase the number of internal hidden neurons?\n",
        "\n",
        "ANS:  increasing the number of hidden neurons increases the performance of a model using the MNIST dataset. The MNIST dataset is a common standard dataset used to evaluate machine learning models performance, which is just a task of recognizing digits from 0 to 9.\n",
        "\n",
        "Q3: What happens when you increase the size of batch computation?\n",
        "\n",
        "ANS: larger batch sizes make larger gradient steps than smaller batch sizes for the same number of samples seen. for the same average Euclidean norm distance from the initial weights of the model, larger batch sizes have larger variance in the distance.\n",
        "\n",
        "Q4: 5. Why we adopt regularization to avoid overfitting?\n",
        "\n",
        "ANS: Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
        "\n",
        "Q5: What are loss and cost functions in deep learning?\n",
        "\n",
        "ANS: the loss function is to capture the difference between the actual and predicted values for a single record whereas cost functions aggregate the difference for the entire training dataset.\n",
        "\n",
        "Q6: What do ou mean by underfitting in neural networks?\n",
        "\n",
        "ANS: Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
        "\n",
        "Q7: Why we use Dropout in Neural Networks?\n",
        "\n",
        "ANS: Dropout layers have been the go-to method to reduce the overfitting of neural networks. It is the underworld king of regularisation in the modern era of deep learning. In this era of deep learning, almost every data scientist must have used the dropout layer at some moment in their career of building neural networks."
      ],
      "metadata": {
        "id": "pEF_MJ3lu6Yr"
      }
    }
  ]
}