{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(DeepLearning assigment no: 14)"
      ],
      "metadata": {
        "id": "oVIKp_j7Vl3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Is it okay to initialize all the weights to the same value as long as that value is selected\n",
        "randomly using He initialization?\n",
        "\n",
        "ANS: The weights attached to the same neuron, continue to remain the same throughout the training. It makes the hidden units symmetric and this problem is known as the symmetry problem. Hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value.\n",
        "\n",
        "Q2:Is it okay to initialize the bias terms to 0?\n",
        "\n",
        "ANS:It is important to note that setting biases to 0 will not create any problems as non-zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron will still be different.\n",
        "\n",
        "Q3: Name three advantages of the ELU activation function over ReLU.\n",
        "\n",
        "ANS: ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
        "ELU is a strong alternative to ReLU.\n",
        "Unlike to ReLU, ELU can produce negative outputs.\n",
        "\n",
        "Q4: In which cases would you want to use each of the following activation functions: ELU, leaky\n",
        "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "\n",
        "ANS: An ELU activation layer performs the identity operation on positive inputs and an exponential nonlinearity on negative inputs. \n",
        "\n",
        "Leaky ReLU has two benefits: It fixes the “dying ReLU” problem, as it doesn't have zero-slope parts. It speeds up training. There is evidence that having the “mean activation” be close to 0 makes training faster.Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training.\n",
        "\n",
        "ReLU function should only be used in the hidden layers. An output layer can be linear activation function in case of regression problems.\n",
        "\n",
        "The tanh function is mainly used classification between two classes. Both tanh and logistic sigmoid activation functions are used in feed-forward nets.hence tanh functions helps in centering the data by bringing mean close to 0 which makes learning for the next .\n",
        "\n",
        "Logistic functions are often used in neural networks to introduce nonlinearity in the model or to clamp signals to within a specified interval.The logistic model is appropriate whenever the total count has an upper limit and the initial growth is exponential. Examples are the spread of rumors and disease in a limited population and the growth of bacteria or human population when resources are limited.\n",
        "\n",
        "softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels. Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the softmax function.\n",
        "What is softmax function used for?\n",
        "The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n",
        "\n",
        "Q5: What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
        "when using a MomentumOptimizer?\n",
        "\n",
        "ANS: If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum.\n",
        "\n",
        "Q6: Name three ways you can produce a sparse model.\n",
        "\n",
        "ANS: we can do so by performing a simple procedure: compress the original data to a lower-dimensional space, learn a dense model in the low-dimensional space, then apply a sparse recovery method to obtain an approximate solution to the problem in the original, high-dimensional space.\n",
        "\n",
        "Q7: Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
        "new instances)?\n",
        "\n",
        "ANS: Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TazEjvX1Vr3b"
      }
    }
  ]
}