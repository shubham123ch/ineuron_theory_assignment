{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(DeepLearning assigment no: 9)"
      ],
      "metadata": {
        "id": "a1B-lnsdYBap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What are the main tasks that autoencoders are used for?\n",
        "\n",
        "ANS: An autoencoder is an unsupervised learning technique for neural networks that learns efficient data representations (encoding) by training the network to ignore signal “noise.” Autoencoders can be used for image denoising, image compression, and, in some cases, even generation of image data.\n",
        "\n",
        "Q2: Suppose you want to train a classifier, and you have plenty of unlabeled training data but\n",
        "only a few thousand labeled instances. How can autoencoders help? How would you\n",
        "proceed?\n",
        "\n",
        "ANS: \n",
        "\n",
        "\n",
        "Q3: If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder?\n",
        "How can you evaluate the performance of an autoencoder?\n",
        "\n",
        "ANS: I evaluate the performance of an autoencoder is by simply visually comparing the input and output images taken from the test set. This is by no means very scientific, but it gives a good idea whether an autoencoder is able to reconstruct the input images. \n",
        "\n",
        "Q4: What are undercomplete and overcomplete autoencoders? What is the main risk of an\n",
        "excessively undercomplete autoencoder? What about the main risk of an overcomplete\n",
        "autoencoder?\n",
        "\n",
        "ANS: An undercomplete autoencoder has no explicit regularization term - we simply train our model according to the reconstruction loss. Thus, our only way to ensure that the model isn't memorizing the input data is the ensure that we've sufficiently restricted the number of nodes in the hidden layer(s).\n",
        "\n",
        "Undercomplete Autoencoder (the focus of this article) — has fewer nodes (dimensions) in the middle compared to Input and Output layers. In such setups, we tend to call the middle layer a “bottleneck.” Overcomplete Autoencoder — has more nodes (dimensions) in the middle compared to Input and Output layers.\n",
        "\n",
        "The main risk of an excessively undercomplete autoencoder is that it may fail to reconstruct the inputs. The main risk of an overcomplete autoencoder is that it may just copy the inputs to the outputs, without learning any useful feature.\n",
        "\n",
        "Q5: How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
        "\n",
        "ANS: The major parts of the autoencoder network structure are the encoder function and the decoder function, which are used to reconstruct the data. The architecture of deep autoencoder consists of three layers: input, hidden, and output layer.\n",
        "\n",
        "An autoencoder with tied weights has decoder weights that are the transpose of the encoder weights; this is a form of parameter sharing, which reduces the number of parameters of the model.\n",
        "\n",
        "Q6: What is a generative model? Can you name a type of generative autoencoder?\n",
        "\n",
        "ANS: Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).\n",
        "\n",
        "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words.\n",
        "\n",
        "Q7: What is a GAN? Can you name a few tasks where GANs can shine?\n",
        "\n",
        "ANS: Generative adversarial networks (GANs) are an exciting recent innovation in machine learning. GANs are generative models: they create new data instances that resemble your training data. For example, GANs can create images that look like photographs of human faces, even though the faces don't belong to any real person.\n",
        "\n",
        "The Different Types of Generative Adversarial Networks (GANs) are: Vanilla GAN. Conditional Gan (CGAN) Deep Convolutional GAN (DCGAN) CycleGAN.\n",
        "\n",
        "Text-to-Image Translation.\n",
        "Face Frontal View Generation.\n",
        "Generate New Human Poses.\n",
        "Photos to Emojis.\n",
        "Face Aging.\n",
        "Super Resolution.\n",
        "Photo Inpainting.\n",
        "Clothing Translation.\n",
        "\n",
        "Q8: What are the main difficulties when training GANs?\n",
        "\n",
        "ANS: However, there exist major challenges in training of GANs, i.e., mode collapse, non-convergence and instability, due to inappropriate design of network architecture, use of objective function and selection of optimization algorithm.\n",
        "\n",
        "GANs are difficult to train. The reason they are difficult to train is that both the generator model and the discriminator model are trained simultaneously in a game. This means that improvements to one model come at the expense of the other model.\n",
        "\n"
      ],
      "metadata": {
        "id": "ek-o8giUYGNs"
      }
    }
  ]
}