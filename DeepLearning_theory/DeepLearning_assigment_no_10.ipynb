{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(DeepLearning assigment no: 10)"
      ],
      "metadata": {
        "id": "D3F7SYUQk5kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What does a SavedModel contain? How do you inspect its content?\n",
        "\n",
        "ANS: A SavedModel contains a complete TensorFlow program, including trained parameters (i.e, tf. Variable s) and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying with TFLite, TensorFlow. js, TensorFlow Serving, or TensorFlow Hub.\n",
        "\n",
        "Q2: When should you use TF Serving? What are its main features? What are some tools you can\n",
        "use to deploy it?\n",
        "\n",
        "ANS: TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.\n",
        "\n",
        "TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs.\n",
        "Step 1: Install the Docker App.\n",
        "\n",
        "Step 2: Pull the TensorFlow Serving Image. docker pull tensorflow/serving. \n",
        "\n",
        "Step 3: Create and Train the Model. \n",
        "\n",
        "Step 4: Save the Model. \n",
        "\n",
        "Step 5: Serving the model using Tensorflow Serving.\n",
        "\n",
        "Step 6: Make a REST request the model to predict.\n",
        "\n",
        "Q3: How do you deploy a model across multiple TF Serving instances?\n",
        "\n",
        "ANS: Train and evaluate your model.\n",
        "\n",
        "Add TensorFlow Serving distribution URI as a package source:\n",
        "\n",
        "Install TensorFlow Serving.\n",
        "\n",
        "Start running TensorFlow Serving.\n",
        "\n",
        "Make REST requests.\n",
        "\n",
        "Q4: When should you use the gRPC API rather than the REST API to query a model served by TF\n",
        "Serving?\n",
        "\n",
        "ANS: gRPC is roughly 7 times faster than REST when receiving data & roughly 10 times faster than REST when sending data for this specific payload. This is mainly due to the tight packing of the Protocol Buffers and the use of HTTP/2 by gRPC.\n",
        "\n",
        "Q5: What are the different ways TFLite reduces a model’s size to make it run on a mobile or\n",
        "embedded device?\n",
        "\n",
        "ANS: TensorFlow Lite is a set of tools that enables on-device machine learning by helping developers run their models on mobile, embedded.\n",
        "\n",
        "Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\n",
        "\n",
        "Q6: What is quantization-aware training, and why would you need it?\n",
        "\n",
        "ANS:Quantization-aware training helps you train DNNs for lower precision INT8 deployment, without compromising on accuracy. This is achieved by modeling quantization errors during training which helps in maintaining accuracy as compared to FP16 or FP32.\n",
        "\n",
        "Q7: What are model parallelism and data parallelism? Why is the latter\n",
        "generally recommended?\n",
        "\n",
        "ANS:  Data parallelism is when you use the same model for every thread, but feed it with different parts of the data; model parallelism is when you use the same data for every thread, but split the model among threads.\n",
        "Model parallelism shards a model (i.e., its layers or tensors) across multiple cores, unlike data parallelism, replicating the same model for all training cores\n",
        "\n",
        "Q8: When training a model across multiple servers, what distribution strategies can you use?\n",
        "How do you choose which one to use?\n",
        "\n",
        "ANS: Distributed training can utilize multiple GPUs/TPUs on the same machine or multiple GPUs/TPUs on multiple devices in a network. Distributed training can be achieved by data-parallelism using synchronous training like Mirrored or Multi Mirrored strategies where the same model is replicated on all the workers.\n",
        "\n",
        "There are three basic strategies to train a model with multiple nodes: Data-parallel training with synchronous updates. Data-parallel training with asynchronous updates. Model-parallel training."
      ],
      "metadata": {
        "id": "52Z53JhOlAvq"
      }
    }
  ]
}