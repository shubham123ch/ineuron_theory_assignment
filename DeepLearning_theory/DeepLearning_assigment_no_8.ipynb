{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(DeepLearning assigment no: 8)"
      ],
      "metadata": {
        "id": "TNypr-cHU6s7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
        "\n",
        "ANS: Setting an RNN to be stateful means that it can build a state across its training sequence and even maintain that state when doing predictions. The benefits of using stateful RNNs are smaller network sizes and/or lower training times.\n",
        "\n",
        "Q2: Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs\n",
        "for automatic translation?\n",
        "\n",
        "ANS: This two-step model, called an Encoderâ€“Decoder, works much better than trying to translate on the fly with a single sequence-to-sequence RNN (like the one represented on the top left), since the last words of a sentence can affect the first words of the translation, so you need to wait until you have heard the whole .\n",
        "\n",
        "Q3: How can you deal with variable-length input sequences? What about variable-length output\n",
        "sequences?\n",
        "\n",
        "ANS: The first and simplest way of handling variable length input is to set a special mask value in the dataset, and pad out the length of each input to the standard length with this mask value set for all additional entries created. Then, create a Masking layer in the model, placed ahead of all downstream layers.\n",
        "\n",
        "Q4:What is beam search and why would you use it? What tool can you use to implement it?\n",
        "\n",
        "ANS: Beam search is the most popular search strategy for the sequence to sequence Deep NLP algorithms like Neural Machine Translation, Image captioning, Chatbots, etc. Beam search considers multiple best options based on beamwidth using conditional probability, which is better than the sub-optimal Greedy search.\n",
        "\n",
        "Q5:What is an attention mechanism? How does it help?\n",
        "\n",
        "ANS: The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all the encoded input vectors, with the most relevant vectors being attributed the highest weights.\n",
        "\n",
        "Q6: What is the most important layer in the Transformer architecture? What is its purpose?\n",
        "\n",
        "ANS: The most important part here is the “Residual Connections” around the layers. This is very important in retaining the position related information which we are adding to the input representation/embedding across the network. The network displayed catastrophic results on removing the Residual Connections.\n",
        "\n",
        "Q7: When would you need to use sampled softmax?\n",
        "\n",
        "ANS: I'd probably consider using sampled softmax if I have over 100,000 classes, or if my final classification layer dominates overall execution time or memory use. An obvious application is large word vocabularies, for example in language modelling."
      ],
      "metadata": {
        "id": "rLxgJt_lVD2d"
      }
    }
  ]
}