{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(NLP assigment no: 1)"
      ],
      "metadata": {
        "id": "7gMQU7qHDWxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Explain One-Hot Encoding.\n",
        "\n",
        "ANS: One-hot encoding in machine learning is the conversion of categorical information into a format that may be fed into machine learning algorithms to improve prediction accuracy. One-hot encoding is a common method for dealing with categorical data in machine learning.\n",
        "\n",
        "Q2: Explain Bag of Words.\n",
        "\n",
        "ANS: The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. \n",
        "\n",
        "Q3: Explain Bag of N-Grams.\n",
        "\n",
        "ANS: A bag-of-n-grams model records the number of times that each n-gram appears in each document of a collection. An n-gram is a collection of n successive words. bagOfNgrams does not split text into words. To create an array of tokenized documents, see tokenizedDocument .\n",
        "\n",
        "Q4: Explain TF-IDF\n",
        "\n",
        "ANS: In information retrieval, tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
        "\n",
        "Q5: What is OOV problem?\n",
        "\n",
        "ANS: Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. In speech recognition, it's the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning.These words that are unknown by the models, known as out-of-vocabulary (OOV) words, need to be properly handled to not degrade the quality of the natural language processing (NLP) applications, which depend on the appropriate vector representation of the texts.\n",
        "\n",
        "Q6: What are word embeddings?\n",
        "\n",
        "ANS: In natural language processing, word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. by using word embeddings, words that are close in meaning are grouped near to one another in vector space. For example, while representing a word such as frog, the nearest neighbour of a frog would be frogs, toads, Litoria.\n",
        "\n",
        "Q7: Explain Continuous bag of words (CBOW)\n",
        "\n",
        "ANS: Continuous Bag of Words Model (CBOW) and Skip-gram\n",
        "In the CBOW model, the distributed representations of context (or surrounding words) are combined to predict the word in the middle . While in the Skip-gram model, the distributed representation of the input word is used to predict the context .\n",
        "CBOW or Continous bag of words is to use embeddings in order to train a neural network where the context is represented by multiple words for a given target words. For example, we could use “cat” and “tree” as context words for “climbed” as the target word.\n",
        "\n",
        "Q8: Explain SkipGram\n",
        "\n",
        "ANS: Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's reverse of CBOW algorithm. Here, target word is input while context words are output.\n",
        "\n",
        "Q9: Explain Glove Embeddings.\n",
        "\n",
        "ANS: GloVe Embeddings are a type of word embedding that encode the co-occurrence probability ratio between two words as vector differences.It is an unsupervised learning algorithm developed by researchers at Stanford University aiming to generate word embeddings by aggregating global word co-occurrence matrices from a given corpus. The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics."
      ],
      "metadata": {
        "id": "h4jbHaJhDkwf"
      }
    }
  ]
}