{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(NLP assigment no: 5)"
      ],
      "metadata": {
        "id": "Zfp7CQ2AhJoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What are Sequence-to-sequence models?\n",
        "\n",
        "ANS: Sequence to Sequence (often abbreviated to seq2seq) models is a special class of Recurrent Neural Network architectures that we typically use (but not restricted) to solve complex Language problems like Machine Translation, Question Answering, creating Chatbots, Text Summarization, etc\n",
        "\n",
        "GPT-3 is basically a transformer model. Transformer models are sequence-to-sequence deep learning models that can produce a sequence of text given an input sequence. These models are designed for text generation tasks such as question-answering, text summarization, and machine translation.\n",
        "\n",
        "Q2: What are the Problem with Vanilla RNNs?\n",
        "\n",
        "ANS: A major issue with the vanilla RNN is that they suffers from vanishing/exploding gradients similarly to issues with deep feedforward networks. At each timestep, the hidden state ht is multiplied by W, at the last timestep, the value of ht is effectively multiplied by W .\n",
        "\n",
        "Q3: What is Gradient clipping?\n",
        "\n",
        "ANS: What is gradient clipping? Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights.Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output.\n",
        "\n",
        "Q4: Explain Attention mechanism\n",
        "\n",
        "ANS: The idea behind the attention mechanism was to permit the decoder to utilize the most relevant parts of the input sequence in a flexible manner, by a weighted combination of all the encoded input vectors, with the most relevant vectors being attributed the highest weights.\n",
        "Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks.\n",
        "\n",
        "Types Of Attention Mechanisms\n",
        "1)Generalized Attention. \n",
        "2)Self-Attention. \n",
        "3)Multi-Head Attention. \n",
        "4)Additive Attention. \n",
        "5)Global Attention. \n",
        "Attention Mechanism Helps In Automating Deep Learning Applications.\n",
        "\n",
        "Q5: Explain Conditional random fields (CRFs)\n",
        "\n",
        "ANS: Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighbouring\" samples, a CRF can take context into account.\n",
        "Conditional random fields (CRFs) are graphical models that can leverage the structural dependencies between outputs to better model data with an underlying graph structure.\n",
        "\n",
        "\n",
        "Q6: Explain self-attention\n",
        "\n",
        "ANS: Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
        "the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\n",
        "\n",
        "Q7: What is Bahdanau Attention?\n",
        "\n",
        "ANS: The Bahdanau attention was proposed to address the performance bottleneck of conventional encoder-decoder architectures, achieving significant improvements over the conventional approach. In this tutorial, you will discover the Bahdanau attention mechanism for neural machine translation.\n",
        "\n",
        "Q8: What is a Language Model?\n",
        "\n",
        "ANS: A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability P to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages.Language modeling (LM) is the use of various statistical and probabilistic techniques to determine the probability of a given sequence of words occurring in a sentence. Language models analyze bodies of text data to provide a basis for their word predictions.\n",
        "\n",
        "Q9: What is Multi-Head Attention?\n",
        "\n",
        "ANS: Multi-head Attention is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension.\n",
        "\n",
        "the main advantage of the multi-head attention is the training stability, since it has less number of layers than the single-head attention, when attending the same number of positions.\n",
        "\n",
        "Q10:What is Bilingual Evaluation Understudy (BLEU)\n",
        "\n",
        "ANS: BLEU is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vH394aO-hObY"
      }
    }
  ]
}