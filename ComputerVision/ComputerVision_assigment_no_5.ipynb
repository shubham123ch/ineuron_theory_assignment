{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(ComputerVision assigment no: 5)"
      ],
      "metadata": {
        "id": "lfc_lVvhW5Vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: 1. How can each of these parameters be fine-tuned?\n",
        "\n",
        " • Number of hidden layers:  Generally, this is done by replacing the last fully-connected layer and training the model while only updating the weights of the linear layers and letting the convolutional layers keep their weights.\n",
        "\n",
        "Network architecture (network depth): Neural Networks consist of layers where each layer has multiple neurons. The number of layers in a neural network defines its depth. Also, a neural network must have at least two layers: Input layer – it brings the input data into the system and represents the beginning of the neural network architecture.\n",
        "\n",
        "Each layer&#39;s number of neurons (layer width): If proceed without convolution then you need 224 x 224 x 3 = 100, 352 numbers of neurons in input layer but after applying convolution you input tensor dimension is reduced to 1 x 1 x 1000. It means you only need 1000 neurons in first layer of feedforward neural network.\n",
        "\n",
        "Form of activation: The activation function is a node that is put at the end of or in between Neural Networks. They help to decide if the neuron would fire or not. “The activation function is the non linear transformation that we do over the input signal. This transformed output is then sent to the next layer of neurons as input.”Binary Step Function. ...\n",
        "Linear Function. ...\n",
        "Sigmoid. ...\n",
        "Tanh. ...\n",
        "ReLU. ...\n",
        "Leaky ReLU. ...\n",
        "Parameterised ReLU. ...\n",
        "Exponential Linear Unit.\n",
        "\n",
        "\n",
        "Optimization and learning: What is optimization learning?\n",
        "Optimization is the process where we train the model iteratively that results in a maximum and minimum function evaluation. It is one of the most important phenomena in Machine Learning to get better results.\n",
        "\n",
        "Learning rate and decay schedule:Learning rate schedules seek to adjust the learning rate during training by reducing the learning rate according to a pre-defined schedule. Common learning rate schedules include time-based decay, step decay and exponential decay.\n",
        "\n",
        "Mini batch size: The amount of data included in each sub-epoch weight change is known as the batch size. For example, with a training dataset of 1000 samples, a full batch size would be 1000, a mini-batch size would be 500 or 200 or 100, and an online batch size would be just 1.\n",
        "\n",
        "Algorithms for optimization: Gradient Descent is the most basic but most used optimization algorithm. It's used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm.\n",
        "\n",
        "The number of epochs (and early stopping criteria): This strategy of stopping early based on the validation set performance is called Early Stopping. This is explained with the below diagram. The validation set accuracy, however, saturates between 8 to 10 epochs. This is where the model can be stopped training.\n",
        "\n",
        "Overfitting that be avoided by using regularization techniques: Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
        "\n",
        "• L2 normalization: the normalization technique that modifies the dataset values in a way that in each row the sum of the squares will always be up to 1. L2 regularization optimizes the mean cost (whereas L1 reduces the median explanation) which is often used as a performance measurement. This is especially good if you know you don't have any outliers and you want to keep the overall error small. The solution is more likely to be unique.\n",
        "\n",
        "Drop out layers: The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
        "\n",
        "Data augmentation: Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model.\n"
      ],
      "metadata": {
        "id": "7bw2CSl3W-P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GbPtNircXA1M"
      }
    }
  ]
}