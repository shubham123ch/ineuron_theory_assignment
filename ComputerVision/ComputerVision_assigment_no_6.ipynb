{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(ComputerVision assigment no: 6)"
      ],
      "metadata": {
        "id": "-Kkis3fHaLJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?\n",
        "\n",
        "ANS:  'trainable parameters' are those which value is modified according to their gradient (the derivative of the error/loss/cost relative to the parameter), whereas 'non-trainable parameters' are those which value is not optimized according to their gradient\n",
        "\n",
        "Q2: In the CNN architecture, where does the DROPOUT LAYER go?\n",
        "\n",
        "ANS: We can apply a Dropout layer to the input vector, in which case it nullifies some of its features; but we can also apply it to a hidden layer, in which case it nullifies some hidden neurons.Dropout can be applied to hidden neurons in the body of your network model. In the example below, Dropout is applied between the two hidden layers and between the last hidden layer and the output layer. \n",
        "\n",
        "Q3:What is the optimal number of hidden layers to stack?\n",
        "\n",
        "ANS: If data is less complex and is having fewer dimensions or features then neural networks with 1 to 2 hidden layers would work. If data is having large dimensions or features then to get an optimum solution, 3 to 5 hidden layers can be used.\n",
        "\n",
        "Q4: In each layer, how many secret units or filters should there be?\n",
        "\n",
        "ANS: The size of the hidden layer is normally between the size of the input and output-. It should be should be 2/3 the size of the input layerplus the size of the o/p layer The number of hidden neurons should be less than twice the size of the input layer.\n",
        "\n",
        "Q5:What should your initial learning rate be?\n",
        "\n",
        "ANS: There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc.\n",
        "\n",
        "Q6: What do you do with the activation function?\n",
        "\n",
        "ANS: The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
        "\n",
        "Q7: What is NORMALIZATION OF DATA?\n",
        "\n",
        "ANS: Normalization is the process of organizing data in a database. This includes creating tables and establishing relationships between those tables according to rules designed both to protect the data and to make the database more flexible by eliminating redundancy and inconsistent dependency.\n",
        "\n",
        "Q8: What is IMAGE AUGMENTATION and how does it work?\n",
        "\n",
        "ANS: Image augmentation is a technique that is used to artificially expand the data-set. This is helpful when we are given a data-set with very few data samples. In case of Deep Learning, this situation is bad as the model tends to over-fit when we train it on limited number of data samples.\n",
        "\n",
        "Q9: What is DECLINE IN LEARNING RATE?\n",
        "\n",
        "ANS: The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights.\n",
        "both low and high learning rates results in wasted time and resources. A lower learning rate means more training time. more time results in increased cloud GPU costs. a higher rate could result in a model that might not be able to predict anything accurately.\n",
        "\n",
        "Q10:What does EARLY STOPPING CRITERIA mean?\n",
        "\n",
        "ANS: In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration."
      ],
      "metadata": {
        "id": "w3Gfrk_kaPnz"
      }
    }
  ]
}