{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(ComputerVision assigment no: 3)"
      ],
      "metadata": {
        "id": "Pb93KLiN9gf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: After each stride-2 conv, why do we double the number of filters?\n",
        "\n",
        "ANS: A stride 2 conv with the default padding (1) and ks (3) will reduce the activation map dimension by half. Formula: (n + 2*pad - ks)//stride + 1. As the activation map dimension reduces by half we double the number of filters. This results in no overall change in computation as the network gets deeper and deeper.\n",
        "\n",
        "Q2: Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
        "\n",
        "ANS: Increasing kernel size means effectively increasing the total number of parameters. So, it is expected that the model has a higher complexity to address a given problem. So it should perform better at least for a particular training set.\n",
        "\n",
        "Q3: What data is saved by ActivationStats for each layer?\n",
        "\n",
        "ANS: ActivationStats saves the layer activations in self.stats for all modules passed to it. By default it will save activations for all modules.\n",
        "\n",
        "Q4: How do we get a learner&#39;s callback after they&#39;ve completed training?\n",
        "\n",
        "ANS: Any tweak of this training loop is defined in a Callback to avoid over-complicating the code of the training loop.\n",
        "\n",
        "Q5: What are the drawbacks of activations above zero?\n",
        "\n",
        "ANS: Zero-centered activation functions ensure that the mean activation value is around zero. This property is important in deep learning because it has been empirically shown that models operating on normalized data––whether it be inputs or latent activations––enjoy faster convergence.\n",
        "\n",
        "Q6: 6.Draw up the benefits and drawbacks of practicing in larger batches?\n",
        "\n",
        "ANS: bigger the batch size, lesser is the noise in the gradients and so better is the gradient estimate. This allows the model to take a better step towards a minima. However, the challenge is that bigger batch size needs more memory and each step is time consuming.\n",
        "\n",
        "Q7: Why should we avoid starting training with a high learning rate?\n",
        "\n",
        "ANS: If your learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in your network. However, if your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.\n",
        "\n",
        "Q8: What are the pros of studying with a high rate of learning?\n",
        "\n",
        "ANS: If your learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in your network. However, if your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.\n",
        "\n",
        "Q9: Why do we want to end the training with a low learning rate?\n",
        "\n",
        "ANS: A reduced learning rate can make this route more smooth, which can enhance generalization accuracy greatly. However, there comes the point where lowering the learning rate any further is simply a waste of time, resulting in many more steps than are required to walk the same path to the same minimum.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1Wr3UZr9ldI"
      }
    }
  ]
}