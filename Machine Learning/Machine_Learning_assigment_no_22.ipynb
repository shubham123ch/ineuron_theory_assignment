{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AgzQ_HthiXo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(Machine_Learning assigment no: 22)"
      ],
      "metadata": {
        "id": "9AFhpoKrhjIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Is there any way to combine five different models that have all been trained on the same training\n",
        "data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is\n",
        "the reason?\n",
        "\n",
        "ANS: The most common method to combine models is by averaging multiple models, where taking a weighted average improves the accuracy. Bagging, boosting, and concatenation are other methods used to combine deep learning models. Stacked ensemble learning uses different combining techniques to build a model.\n",
        "\n",
        "Q2: What&#39;s the difference between hard voting classifiers and soft voting classifiers?\n",
        "\n",
        "ANS: Hard voting entails picking the prediction with the highest number of votes, whereas soft voting entails combining the probabilities of each prediction in each model and picking the prediction with the highest total probability.\n",
        "\n",
        "Q3: It is quite possible to speed up training of a bagging ensemble, pasting ensembles and Random Forests by distributing it across multiple servers, since each predictor in the ensemble is independent of the others.\n",
        "\n",
        "Q4: What is the advantage of evaluating out of the bag?\n",
        "\n",
        "ANS: Better Predictive Model: OOB_Score helps in the least variance and hence it makes a much better predictive model than a model using other validation techniques. Less Computation: It requires less computation as it allows one to test the data as it is being trained.\n",
        "\n",
        "Q5: What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra\n",
        "randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random\n",
        "Forests?\n",
        "\n",
        "ANS: Random Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. Therefore, Extra Trees adds randomization but still has optimization.\n",
        "\n",
        "Q6: Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training\n",
        "data?\n",
        "\n",
        "ANS: If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how? try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator, also try slightly increasing the learning rate.\n",
        "\n",
        "Q7: Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the\n",
        "training set?\n",
        "\n",
        "ANS: If your gradient boosting ensemble overfits the training set, you should try decreasing the learning rate. You could also use early stopping to find the right number of predictors (you probably have too many)."
      ],
      "metadata": {
        "id": "qOFwxSCohpW9"
      }
    }
  ]
}