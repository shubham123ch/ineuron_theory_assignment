{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I69-821hj7EV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(Machine_Learning assigment no: 23)"
      ],
      "metadata": {
        "id": "-WBBcAelj7s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What are the key reasons for reducing the dimensionality of a dataset? What are the major\n",
        "disadvantages?\n",
        "\n",
        "ANS: As the number of features increases, the number of samples also gets increased proportionally, and the chance of overfitting also increases. If the machine learning model is trained on high-dimensional data, it becomes overfitted and results in poor performance.\n",
        "\n",
        "Q2: What is the dimensionality curse?\n",
        "\n",
        "ANS: The curse of dimensionality, first introduced by Bellman [1], indicates that the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.\n",
        "\n",
        "Q3: Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\n",
        "can you go about doing it? If not, what is the reason?\n",
        "\n",
        "ANS: No, dimensionality reduction is not reversible in general. It loses information.\n",
        "\n",
        "Q4: Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
        "\n",
        "ANS: PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear because it can at least get rid of useless dimensions. However, if there are no useless dimensions, reducing dimensionality with PCA will lose too much information.\n",
        "\n",
        "Q5: Assume you&#39;re running PCA on a 1,000-dimensional dataset with a 95 percent explained variance\n",
        "ratio. What is the number of dimensions that the resulting dataset would have?\n",
        "\n",
        "ANS: hard to say, it depends on dataset.\n",
        "\n",
        "Q6: Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
        "\n",
        "ANS: Vanilla PCA: the dataset fit in memory.\n",
        "\n",
        "Incremental PCA: larget dataset that don't fit in memory, online taks.\n",
        "\n",
        "Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
        "\n",
        "kenrl PCA: used for nonlinear PCA.\n",
        "\n",
        "Q7: How do you assess a dimensionality reduction algorithm&#39;s success on your dataset?\n",
        "\n",
        "ANS:  A dimensionality reduction algorithm is said to work well if it eliminates a significant number of dimensions from the dataset without losing too much information.\n",
        "\n",
        "Q8:Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
        "\n",
        "ANS: It can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE(Locally-Linear Embedding)."
      ],
      "metadata": {
        "id": "7jbYYXNukBkD"
      }
    }
  ]
}