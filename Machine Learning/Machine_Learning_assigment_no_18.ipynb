{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Shubham Kumar Chaturvedi(Machine_Learning assigment no: 18)"
      ],
      "metadata": {
        "id": "_C0cscqgcHZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is the difference between supervised and unsupervised learning? Give some examples to\n",
        "illustrate your point.\n",
        "\n",
        "ANS: Supervised Learning includes various algorithms such as Bayesian Logic, Decision Tree, Logistic Regression, Linear Regression, Multi-class Classification, Support Vector Machine etc. Unsupervised Learning includes various algorithms like KNN, Apriori Algorithm, and Clustering.\n",
        "\n",
        "Q2: Mention a few unsupervised learning applications.\n",
        "\n",
        "ANS: Unsupervised learning finds a myriad of real-life applications, including:\n",
        "data exploration,\n",
        "customer segmentation,\n",
        "recommender systems,\n",
        "target marketing campaigns, and.\n",
        "data preparation and visualization, etc.\n",
        "\n",
        "Q3: What are the three main types of clustering methods? Briefly describe the characteristics of each.\n",
        "\n",
        "ANS: Centroid-based Clustering.\n",
        "\n",
        "Density-based Clustering.\n",
        "\n",
        "Distribution-based Clustering.\n",
        "Hierarchical Clustering.\n",
        "\n",
        "Q4: Explain how the k-means algorithm determines the consistency of clustering.\n",
        "\n",
        "ANS: In k-means clustering, the number of clusters that you want to divide your data points into i.e., the value of K has to be pre-determined whereas in Hierarchical clustering data is automatically formed into a tree shape form (dendrogram).\n",
        "\n",
        "Q5: With a simple illustration, explain the key difference between the k-means and k-medoids\n",
        "algorithms.\n",
        "\n",
        "ANS: K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means algorithm, k -medoids chooses datapoints as centers ( medoids or exemplars).\n",
        "\n",
        "Q6: What is a dendrogram, and how does it work? Explain how to do it.\n",
        "\n",
        "ANS: A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes. To avoid crossing lines, the diagram is graphically arranged so that members of each pair of classes to be merged are neighbors in the diagram. The Dendrogram tool uses a hierarchical clustering algorithm.\n",
        "\n",
        "Q7: What exactly is SSE? What role does it play in the k-means algorithm?\n",
        "\n",
        "ANS: Intra-cluster variance (a.k.a., the squared error function or sum of squares within (SSW) or sum of squares error (SSE)) is used to quantify internal cohesion. It is defined as the sum of the squared distance between the average point (called Centroid) and each point of the cluster.\n",
        "\n",
        "Q8: With a step-by-step algorithm, explain the k-means procedure.\n",
        "\n",
        "ANS: Select the Number of Clusters, k.\n",
        "\n",
        "Step 2: Select k Points at Random. \n",
        "\n",
        "Step 3: Make k Clusters. \n",
        "\n",
        "Step 4: Compute New Centroid of Each Cluster. \n",
        "\n",
        "Step 5: Assess the Quality of Each Cluster. \n",
        "\n",
        "Step 6: Repeat Steps 3â€“5.\n",
        "\n",
        "Q9:In the sense of hierarchical clustering, define the terms single link and complete link.\n",
        "\n",
        "ANS: In single-link (or single linkage) hierarchical clustering, we merge in each step the two clusters whose two closest members have the smallest distance (or: the two clusters with the smallest minimum pairwise distance). Complete-link clustering can also be described using the concept of clique.\n",
        "\n",
        "Q10: How does the apriori concept aid in the reduction of measurement overhead in a business\n",
        "basket analysis? Give an example to demonstrate your point.\n",
        "\n",
        "ANS: Algorithms that use association rules include AIS, SETM and Apriori. The Apriori algorithm is commonly cited by data scientists in research articles about market basket analysis. It identifies frequent items in the database and then evaluates their frequency as the datasets are expanded to larger sizes."
      ],
      "metadata": {
        "id": "EreCmj9TcJZ4"
      }
    }
  ]
}